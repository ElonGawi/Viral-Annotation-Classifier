{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed3b187",
   "metadata": {},
   "source": [
    "Notebook containing the final versions of models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc542b9c",
   "metadata": {},
   "source": [
    "I will try and describe what I have done and why my final models look like they do. I quickly want to note something- when I say accracy is increased/decreased, I am actually refering to another value. I look at a combination of Accuracy, Macro Average, and Weighted Average from the classification_report output. The data we have is drastically skewed toward proper annotations, and by only looking at accuracy we might miss that a model which is really good at predicting proper annotations actually suck for the low and uninformative ones. Macro average is the unweighted mean of per-class scores making it useful when analysing imbalanced datasets. I need to read up on weighted average more, have honestly not looked at it much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefad0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading annotation and label file\n",
    "data = pd.read_csv(r'.\\AF50m_subset_REGEX_man_labels_5k.txt', sep=\"\\t\")\n",
    "\n",
    "# Getting the annotatons that have been labeled manually\n",
    "labeled = data.loc[data[\"manual_label\"].notna(), [\"protein_annotation\", \"manual_label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c1878",
   "metadata": {},
   "source": [
    "My main concern for the cleaning function was what tokens do I want to preserve. Take \"Si:ch211-256e16.3\", \"5'(3')-deoxyribonucleotidase\", \"[NAD(P)H]\" as an example. Do I keep These completley intact? The automised tokenization pattern in TfidfVectorizer is r”(?u)\\b\\w\\w+\\b”, which considers tokens as something with atleast two alphanumeric characters, so ignores punctuation etc. In the end, I decided to remove brackets and parenthases, and replace non alphanumeric charactes with blankspaces. This method will not keep specialized words such as atoms or accession codes etc intact, but this approach seems to increase accuracy. I think this mainly is for the low and uninformative class, where accession codes and such aren't as frequent. Essentially a noise reduction step. Also remove any words with less than two characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1bdbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning function\n",
    "\n",
    "# should consider unseen data passed, should we force everything to str and check for naans and non ascii characters?\n",
    "\n",
    "def cleaner(text):\n",
    "    \"\"\"\n",
    "    Takes a string as an input and performs the following operations:\n",
    "        - Lowercases the text\n",
    "        - Replaces []() with \"\"\n",
    "        - Replaces any non-alphanumeric characters with a blankspace\n",
    "        - Removes words with less than 2 characters\n",
    "    Returns the cleaned text.\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r\"[\\[\\]\\(\\)]\", \"\", text)  # removing brackets etc\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)  # remove non-alphanumeric characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # remove multiple spaces\n",
    "\n",
    "    # Remove words shorter than 2 characters\n",
    "    text = re.sub(r\"\\b\\w{1}\\b\", \"\", text)  # Removes isolated 1-character tokens\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # cleans up extra spaces again\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de43edc5",
   "metadata": {},
   "source": [
    "Lemmatization reduces a word to it's base form. Here, I try and keep words with numers in the intact, but remove digits completley, as these do not seem to indicative on the informativeness of an annotation. I did also try a version where the cleaner maintains any words with special characters such as .:-_'+ and the lemmatizer also considers words with these as tokens, but ultimatley found that removing these characters increases accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d373a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization function\n",
    "\n",
    "# Here I need to think about how things should be loaded for the package\n",
    "# e.g. do I put load inside the function or outside?\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n",
    "\n",
    "# Pattern for tokens that contain both letters and digits\n",
    "HAS_LETTER_AND_DIGIT = re.compile(r'(?=.*[a-zA-Z])(?=.*\\d)')\n",
    "\n",
    "def lemmatizer(text):\n",
    "    \"\"\"\n",
    "    Lemmatizes the input text using spaCy. Removes spaces, punctuation, and pure numbers.\n",
    "    Keeps alphanumeric tokens (those containing both letters and digits).\n",
    "    Returns the lemmatized text as a single string.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    for tok in doc:\n",
    "        if tok.is_space:\n",
    "                continue\n",
    "        t = tok.text\n",
    "        if tok.is_alpha:\n",
    "            lemmas.append(tok.lemma_.lower()) #eg binding -> bind\n",
    "        elif HAS_LETTER_AND_DIGIT.search(t):\n",
    "            # Keep alphanumeric tokens like asp45 or hsp70\n",
    "            lemmas.append(t.lower())\n",
    "        # else: skip pure numbers and punctuation (shouldn’t occur post-cleaning)\n",
    "\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d4af1",
   "metadata": {},
   "source": [
    "For vectorisation I tried quite a few things. \n",
    "\n",
    "Stopwords are words that occur frequently but contribute very little to the meaning of a sentence. The most frequent word in our dataset in protein, which appears in more than half of the annotations. There is a high drop-off in frequency after this, with the second most frequent word, domain, only appearing in 14.6% of annotations. Ultimately, I found just using the english stopwords is fine. \n",
    "\n",
    "Min_df defines the minimum amount of documents a word must appear in. I found that models have the highest accuracy when there is no min_df, that is to say by allowing tokens to appear in only one document.\n",
    "\n",
    "ngram_range defines how many n-grams are used. So as an exmple, say we have the text \"proten domain\" and ngram_range=(1,2), that means we get three tokens, \"protein\", \"domain\", and \"protein domain\". Due to the harsh removing all special characters, I tried different n-gram ranges (I tried 1-4, could expand if interesting), and found that (1,2) was the best. \n",
    "\n",
    "I also tried a combination of a cleaner allowing special characters and a tokenizer allowing these, but ultimately found that these models had worse accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a293801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer\n",
    "\n",
    "# Testing without min_df\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=False,\n",
    "    stop_words=list(ENGLISH_STOP_WORDS), \n",
    "    ngram_range=(1,2), \n",
    "    max_df=0.9 # words that appear in more than 90% of annotations are ignored\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and test sets\n",
    "\n",
    "x = labeled[\"protein_annotation\"].apply(cleaner)\n",
    "x = x.apply(lemmatizer)\n",
    "y = labeled[\"manual_label\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(x_train)\n",
    "test_vectors = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d935bca",
   "metadata": {},
   "source": [
    "The classifiers I have tried are Logistic regression, SVM, Random forests, and two Naive Bayes methods. I also want to try and use a simple neural network, but it did not go well and is a future problem. \n",
    "\n",
    "I will now write the ChatGPT summary  of these models strengths and weaknesses, but I do want to find sources on this seperately at some point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af98449",
   "metadata": {},
   "source": [
    "Logistic regression is fast to train, relatively easy to interpre, and works for multiclass classification. it is however a linear model, so will not capture any non-linear trends in the data. It may also struggle if the classes are imbalanced without proper handling, and may struggle more with sparse data than other text algorithms. \n",
    "\n",
    "Below is the model which I found had highest macro average and accuracy after tuning parameters. I cannot say if this is the best possible model, but it is what it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best logistic regression model \n",
    "\n",
    "lr = LogisticRegression(\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=1000,\n",
    "    penalty='l2',\n",
    "    C=13.1451\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811c0d6",
   "metadata": {},
   "source": [
    "Support vector machines preform well with sparse high dimensional tasks, such as text classification. It is however harder to interpret and requirs tuning, so may be slower to train than logistic models. \n",
    "\n",
    "I again base this model on parameter tuning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f074ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best SVM model\n",
    "\n",
    "svm = LinearSVC(\n",
    "    C = 0.7743, \n",
    "    class_weight='balanced', \n",
    "    loss= 'squared_hinge', \n",
    "    penalty= 'l2'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3641d6",
   "metadata": {},
   "source": [
    "Random forests can capture complex patterns, not just linear ones, and are generally robust to overfitting. Random forests are however less efficient for sparse data, the decision trees may stuggle to find meaningful splits due to sparse features. Training time and memory may be large, and interpretability is lower. \n",
    "\n",
    "The model I have here was based on just a straight up random forest. I am trying to reduce dimensionality of the vectors and then apply a random forest, the tuning of this is taking ages, will update when there is something to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best RF model\n",
    "rf = RandomForestClassifier(\n",
    "    bootstrap=True,\n",
    "    class_weight='balanced',\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=11,\n",
    "    n_estimators=910,\n",
    "    random_state=42,\n",
    "    n_jobs=-1   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7b178",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes was designed for count data, like the word frequency counts of TF-IDF. It is fast to train and tends to do well assuing independence of features. This independency assumption is however weak for texts, and the model may struggle with skewed classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a47dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best multinomial NB model\n",
    "\n",
    "mnb = MultinomialNB(\n",
    "    alpha=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8f26b",
   "metadata": {},
   "source": [
    "Complement naive bayes is a NB variant designed to handle imbalanced text classification tasks, while retaining NB advantages (may be robust if one class is very large like proper). Similar downsides to MNB, and is less understood in text classification frameworks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400726d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best complement NB model\n",
    "\n",
    "cnb = ComplementNB(\n",
    "    alpha=0.007743)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb47b51",
   "metadata": {},
   "source": [
    "This text is for an eventual neural network. these offer flexibility and may notice patterns linear models miss. These however tend to require more data to train and may otherwise overfit, it is slower with more parameters to tune, weaker interpretability. May need to reduce dimensionality like in random forests. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f9a4a",
   "metadata": {},
   "source": [
    "Finally, the following code is a simple setup for the final models. I need to see about reducing dimensions in Random forests, this might requir some extra steps. When I am happy with my models, the next steps are as follows: talk to group about how we want these models to be accessible and how to compare accuracy in all models, as well as computational costs, and look up how I can make the pretraned models obtainable for others without them needing to retrain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training classifier\n",
    "\n",
    "classifier.fit(train_vectors, y_train)\n",
    "\n",
    "y_pred = classifier.predict(test_vectors)\n",
    "print(classification_report(y_test, y_pred))    \n",
    "\n",
    "class_names = classifier.classes_\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap=\"Blues\", values_format='d')  # 'd' = integer format\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
